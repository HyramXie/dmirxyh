import json
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from collections import defaultdict, Counter
import sys

logger_file = "/root/user/xyh/ProcessDataset/GetFinalResult/MELD/result_report.txt"
json_file = "/root/user/xyh/LLaMA-Factory-main/eval/MELD/generated_predictions.json"
wrong_case_file = "/root/user/xyh/ProcessDataset/GetFinalResult/MELD/case_study.json"

class Logger:
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, "w", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)   # åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯
        self.log.write(message)        # åŒæ—¶å†™å…¥æ–‡ä»¶

    def flush(self):
        self.terminal.flush()
        self.log.flush()

sys.stdout = Logger(logger_file)

# è¯»å– jsonl æ–‡ä»¶


y_true = []
y_pred = []
wrong_cases = []

with open(json_file, 'r', encoding='utf-8') as f:
    for line in f:
        data = json.loads(line)
        # å»é™¤æ ‡ç­¾ä¸­çš„æ¢è¡Œç¬¦ä¸å¤šä½™ç©ºæ ¼
        true_label = data["label"].strip()
        pred_label = data["predict"].strip()
        y_true.append(true_label)
        y_pred.append(pred_label)

        #è·å–é”™è¯¯æ ·æœ¬
        if true_label != pred_label:
            wrong_cases.append({
                "prompt": data["prompt"],
                "predict": data["predict"].strip(),
                "label": data["label"].strip()
            })

# è®¡ç®—å„é¡¹æŒ‡æ ‡
acc = accuracy_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred, average='macro')
prec = precision_score(y_true, y_pred, average='macro')
rec = recall_score(y_true, y_pred, average='macro')

weighted_f1 = f1_score(y_true, y_pred, average='weighted')
weighted_prec = precision_score(y_true, y_pred, average='weighted')
weighted_rec = recall_score(y_true, y_pred, average='weighted')

# æ‰“å°ç»“æœ
print(f"Accuracy: {acc:.4f}")
print(f"Macro F1: {f1:.4f}")
print(f"Macro Precision: {prec:.4f}")
print(f"Macro Recall: {rec:.4f}")
print(f"Weighted F1: {weighted_f1:.4f}")
print(f"Weighted Precision: {weighted_prec:.4f}")
print(f"Weighted Recall: {weighted_rec:.4f}")

# with open(wrong_case_file, "w", encoding="utf-8") as f:
#     json.dump(wrong_cases, f, ensure_ascii=False, indent=2)
# print("finish")



# -----------------------
# ç»Ÿè®¡ï¼šæŒ‰çœŸå®æ ‡ç­¾åˆ†ç»„åˆ†æé”™è¯¯
# -----------------------

total_per_class = Counter(y_true)       # æ¯ä¸ªçœŸå®ç±»åˆ«çš„æ€»æ ·æœ¬æ•°
wrong_per_class = Counter()              # æ¯ä¸ªçœŸå®ç±»åˆ«çš„é”™è¯¯æ•°
confusions = defaultdict(Counter)       # æ¯ç±»â€œé”™æˆäº†ä»€ä¹ˆâ€

for case in wrong_cases:
    true_label = case["label"]
    pred_label = case["predict"]
    wrong_per_class[true_label] += 1
    confusions[true_label][pred_label] += 1

# æŒ‰é”™è¯¯ç‡æ’åº
error_report = []

for label in total_per_class:
    total = total_per_class[label]
    wrong = wrong_per_class[label]
    error_rate = wrong / total
    error_report.append((label, total, wrong, error_rate))

error_report.sort(key=lambda x: x[3], reverse=True)

# -----------------------
# æ‰“å°æŠ¥å‘Š
# -----------------------

print("\nğŸ“Š å„ç±»åˆ«é”™è¯¯ç‡æ’è¡Œæ¦œï¼ˆæŒ‰çœŸå®æ ‡ç­¾ç»Ÿè®¡ï¼‰ï¼š")
print("-" * 60)
print(f"{'Class':15s} {'Total':>8s} {'Wrong':>8s} {'Error Rate':>12s}")
print("-" * 60)

for label, total, wrong, rate in error_report:
    print(f"{label:15s} {total:8d} {wrong:8d} {rate:11.2%}")

# -----------------------
# æ‰“å°ï¼šæ¯ä¸ªç±»åˆ«æœ€å¸¸è§çš„é”™è¯¯é¢„æµ‹
# -----------------------

print("\nğŸ” æ¯ä¸ªç±»åˆ«æœ€å¸¸è¢«é”™è®¤æˆçš„æ ‡ç­¾ï¼ˆTop Errorsï¼‰ï¼š")
print("-" * 60)

for label, preds in confusions.items():
    most_common = preds.most_common(3)
    if most_common:
        print(f"\nçœŸå®ç±»åˆ«: {label}, æ€»å…±æ ·æœ¬æ•°é‡ï¼š{total_per_class[label]}")
        for wrong_label, cnt in most_common:
            print(f"  â†’ è¢«é¢„æµ‹ä¸º {wrong_label}: {cnt} æ¬¡")
