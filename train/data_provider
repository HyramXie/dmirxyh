import os
import json
import torch
from torch.utils.data import Dataset
from transformers import Qwen2_5_VLProcessor
from qwen_vl_utils import process_vision_info 

class MIntRecDataset(Dataset):
    def __init__(self, json_path, video_dir, processor):
        """
        json_path: MIntRec 的标注文件 (train.json)
        video_dir: 视频文件夹路径
        """
        with open(json_path, 'r') as f:
            self.data = json.load(f)
        
        self.video_dir = video_dir
        self.processor = processor
        
        # MIntRec 意图标签列表 (示例)
        self.intents = [
            "complain", "praise", "apologize", "thank", "criticize", 
            "agree", "disagree", "ask", "answer", "taunt" # 请根据实际数据集补充完整
        ]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        messages = item['messages']
        text = messages[0]["content"]
        label = messages[1]["content"] # 假设 label 是意图字符串
        
        video_path = item["video"][0]
        
        # 构造对话格式
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "video",
                        "video": video_path,
                        "max_pixels": 360 * 420, # 控制分辨率以节省显存
                        "fps": 2.0, # 抽帧率
                    },
                    {"type": "text", "text": f"Analyze the speaker's intent in the video and text: '{text}'. Answer with the intent label directly."}
                ],
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": label}]
            }
        ]
        
        # 预处理输入 (Input)
        text_input = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=False
        )
        
        # 提取 Vision Info
        image_inputs, video_inputs = process_vision_info(messages)
        
        return {
            "text_input": text_input,
            # "images": image_inputs,
            "videos": video_inputs,
            "messages": messages # 保留原始 msg 用于 debug 或 collate
        }

class DataCollatorForQwenMIntRec:
    def __init__(self, processor):
        self.processor = processor

    def __call__(self, batch):
        texts = [x["text_input"] for x in batch]
        # images = [x["images"] for x in batch]
        videos = [x["videos"] for x in batch]
        
        # 使用 Processor 统一处理 Padding 和 Tensor 转换
        inputs = self.processor(
            text=texts,
            # images=images,
            videos=videos,
            padding=True,
            return_tensors="pt",
        )
        
        # 创建 Labels (自回归训练)
        # Qwen 的 labels 逻辑：对 input_ids 移位计算 loss
        # 这里为了简单，我们直接把 input_ids 复制给 labels
        # 并在 Trainer 中利用 DataCollatorForSeq2Seq 的逻辑，或者手动 mask 掉 user 部分
        # Qwen2.5-VL 默认行为通常不需要额外 mask，如果使用 chat template
        
        labels = inputs["input_ids"].clone()
        # 简单的 Mask 策略：把 pad token 设为 -100
        labels[labels == self.processor.tokenizer.pad_token_id] = -100
        
        # 注意：严格来说应该把 "User" 指令部分的 Label 设为 -100，
        # 但 Qwen2.5-VL 的 training recipe 比较鲁棒，全序列训练通常也没问题，
        # 或者你可以使用 trl 库的 DataCollatorForCompletionOnlyLM 来精确 mask。
        
        inputs["labels"] = labels
        return inputs